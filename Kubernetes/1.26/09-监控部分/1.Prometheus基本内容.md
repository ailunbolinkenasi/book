# Prometheus

我们知道监控是保证系统运行必不可少的功能，特别是对于 Kubernetes 这种比较庞大的系统来说，监控报警更是不可或缺，我们需要时刻了解系统的各种运行指标，也需要时刻了解我们的 Pod 的各种指标，更需要在出现问题的时候有报警信息通知到我们。

在早期的版本中 Kubernetes 提供了 heapster、influxDB、grafana 的组合来监控系统，在现在的版本中已经移除掉了 heapster，现在更加流行的监控工具是 [Prometheus](https://prometheus.io/)，Prometheus 是 Google 内部监控报警系统的开源版本，是 Google SRE 思想在其内部不断完善的产物，它的存在是为了更快和高效的发现问题，快速的接入速度，简单灵活的配置都很好的解决了这一切，而且是已经毕业的 CNCF 项目。

## 简介

Prometheus 最初是 SoundCloud 构建的开源系统监控和报警工具，是一个独立的开源项目，于2016年加入了 CNCF 基金会，作为继 Kubernetes 之后的第二个托管项目。Prometheus 相比于其他传统监控工具主要有以下几个特点：

- 具有由 metric 名称和键/值对标识的时间序列数据的多维数据模型
- 有一个灵活的查询语言
- 不依赖分布式存储，只和本地磁盘有关
- 通过 HTTP 的服务拉取时间序列数据
- 也支持推送的方式来添加时间序列数据
- 还支持通过服务发现或静态配置发现目标
- 多种图形和仪表板支持

Prometheus 由多个组件组成，但是其中有些组件是可选的：

- `Prometheus Server`：用于抓取指标、存储时间序列数据
- `exporter`：暴露指标让任务来抓
- `pushgateway`：push 的方式将指标数据推送到该网关
- `alertmanager`：处理报警的报警组件 `adhoc`：用于数据查询

大多数 Prometheus 组件都是用 Go 编写的，因此很容易构建和部署为静态的二进制文件。下图是 Prometheus 官方提供的架构及其一些相关的生态系统组件：

![Prometheus架构图](https://bxdc-static.oss-cn-beijing.aliyuncs.com/images/20220402181610.png)

## 安装

由于 Prometheus 是 Golang 编写的程序，所以要安装的话也非常简单，只需要将二进制文件下载下来直接执行即可

- 前往地址：https://prometheus.io/download 下载最新版本即可。

Prometheus 是通过一个 YAML 配置文件来进行启动的，如果我们使用二进制的方式来启动的话，可以使用下面的命令：

```shell
./prometheus --config.file=prometheus.yml
```

其中 `prometheus.yml` 文件的基本配置如下：

```yaml
global:
  scrape_interval:     15s
  evaluation_interval: 15s

rule_files:
  # - "first.rules"
  # - "second.rules"

scrape_configs:
  - job_name: prometheus
    static_configs:
      - targets: ['localhost:9090']
```

上面这个配置文件中包含了3个模块：`global`、`rule_files` 和 `scrape_configs`。

- ```
  global
  ```

   

  模块控制

   

  ```
  Prometheus Server
  ```

   

  的全局配置：

  - `scrape_interval`：表示 prometheus 抓取指标数据的频率，默认是15s，我们可以覆盖这个值
  - `evaluation_interval`：用来控制评估规则的频率，prometheus 使用规则产生新的时间序列数据或者产生警报

- `rule_files`：指定了报警规则所在的位置，prometheus 可以根据这个配置加载规则，用于生成新的时间序列数据或者报警信息，当前我们没有配置任何报警规则。

- `scrape_configs` 用于控制 prometheus 监控哪些资源。

由于 prometheus 通过 HTTP 的方式来暴露的它本身的监控数据，prometheus 也能够监控本身的健康情况。在默认的配置里有一个单独的 job，叫做 prometheus，它采集 prometheus 服务本身的时间序列数据。这个 job 包含了一个单独的、静态配置的目标：监听 localhost 上的 9090 端口。prometheus 默认会通过目标的 `/metrics` 路径采集 metrics。所以，默认的 job 通过 URL：`http://localhost:9090/metrics` 采集 metrics。收集到的时间序列包含 prometheus 服务本身的状态和性能。如果我们还有其他的资源需要监控的话，直接配置在 `scrape_configs` 模块下面就可以了。

### 示例应用

比如我们在本地启动一些样例来让 Prometheus 采集。Go 客户端库包含一个示例，该示例为具有不同延迟分布的三个服务暴露 RPC 延迟。

首先确保已经安装了 Go 环境并启用 go modules，下载 Prometheus 的 Go 客户端库并运行这三个示例：

```shell
git clone https://github.com/prometheus/client_golang.git
cd client_golang/examples/random
export GO111MODULE=on
export GOPROXY=https://goproxy.cn
go build
```

然后在3个独立的终端里面运行3个服务：

```shell
./random -listen-address=:8080
./random -listen-address=:8081
./random -listen-address=:8082
```

这个时候我们可以得到3个不同的监控接口：

- http://localhost:8080/metrics
- http://localhost:8081/metrics
- http://localhost:8082/metrics

然后可以修改`prometheus.yml`来设置刚刚我们启动的几个监控端口

```yaml
scrape_configs:
  - job_name: 'example-random'
    # Override the global default and scrape targets from this job every 5 seconds.
    scrape_interval: 5s
    static_configs:
      - targets: ['localhost:8080', 'localhost:8081']
        labels:
          group: 'production'
      - targets: ['localhost:8082']
        labels:
          group: 'canary'
```

为了能够方便的管理配置文件，我们这里将 `prometheus.yml` 文件用 ConfigMap 的形式进行管理：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
```

配置文件创建完成了，以后如果我们有新的资源需要被监控，我们只需要将上面的 ConfigMap 对象更新即可。现在我们来创建 prometheus 的 Pod 资源，按理说正常会用`StatefulSet`进行部署

- 有需要了解`nfs-client-provisioner`：[Github](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)

> 注意：这里我们用了`nfs-client-provisioner`来动态的分配`Pv`以及`Pvc`存储来持久化数据卷。

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: prometheus-k8s
  labels:
    app: prometheus-k8s
spec:
  serviceName: prometheus
  replicas: 2
  selector:
    matchLabels:
      app: prometheus-k8s
  template:
    metadata:
      labels:
        app: prometheus-k8s
    spec:
      containers:
      - name: prometheus-k8s
        image: prom/prometheus:v2.30.3
        args:
        - "--config.file=/etc/prometheus/prometheus.yml"
        - "--storage.tsdb.path=/data" # 指定Tsdb的数据存储目录
        - '--web.console.templates=/etc/prometheus/consoles' 
        - '--web.console.libraries=/etc/prometheus/console_libraries'
        - '--storage.tsdb.retention.time=7d'
        - '--web.enable-lifecycle' # 支持热更新
        - '--query.max-concurrency=1000'
        - '--web.route-prefix=/'
        ports:
          - name: web
            containerPort: 9090
            protocol: TCP
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus
        - name: prometheus-data
          mountPath: /data
        resources:
          requests:
            cpu: 100m
            memory: 512Mi
          limits:
            cpu: 100m
            memory: 512Mi
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
  volumeClaimTemplates:
    - metadata:
        name: prometheus-data
        annotations:
          volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"
      spec:
        accessModes: ["ReadWriteMany"]
        resources:
          requests:
            storage: 20Gi
---
apiVersion: v1
kind: ConfigMap
metadata: 
  name: prometheus-config
  labels:
    app: prometheus
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s
      scrape_timeout: 15s
    scrape_configs:
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
spec:
  selector:
    app: prometheus-k8s
  ports:
  - name: http
    port: 9090
    targetPort: 9090
  clusterIP: None
```

## 监控一些Kubernetes的应用

Prometheus 的数据指标是通过一个公开的 HTTP(S) 数据接口获取到的，我们不需要单独安装监控的 agent，只需要暴露一个 metrics 接口，Prometheus 就会定期去拉取数据；对于一些普通的 HTTP 服务，我们完全可以直接重用这个服务，添加一个 `/metrics` 接口暴露给 Prometheus；而且获取到的指标数据格式是非常易懂的，不需要太高的学习成本。

现在很多服务从一开始就内置了一个 `/metrics` 接口，比如 Kubernetes 的各个组件、istio 服务网格都直接提供了数据指标接口。有一些服务即使没有原生集成该接口，也完全可以使用一些 `exporter` 来获取到指标数据，比如 `mysqld_exporter`、`node_exporter`，这些 `exporter` 就有点类似于传统监控服务中的 agent，作为服务一直存在，用来收集目标服务的指标数据然后直接暴露给 Prometheus。

### 普通应用

对于普通应用只需要能够提供一个满足 prometheus 格式要求的 `/metrics` 接口就可以让 Prometheus 来接管监控，比如 Kubernetes 集群中非常重要的 CoreDNS 插件，一般默认情况下就开启了 `/metrics` 接口：

```bash
[root@Online-Beijing-master1 ~]# kubectl get cm coredns -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153  # 默认开启了Prometheus
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        template ANY AAAA {
           rcode NXDOMAIN
        }
        cache 30
        loop
        reload
        loadbalance
    }
```

基于这种情况我们可以直接配置`configmap`,因为支持热更新

- 以`volume`挂载方式进行的配置文件会支持热更新,但是具体应用是否取决于热更新要取决于应用
- 如果需要热更新请了解`Prometheus-Reload`

```yaml
global:
  scrape_interval: 15s
  scrape_timeout: 15s
scrape_configs:
- job_name: 'prometheus'
  static_configs:
  - targets: ['localhost:9090']
- job_name: 'CoreDns'
  static_configs:
  - targets: ['10.10.180.70:9153'] 
```

### 使用exporter监控

- 关于更多地`exporter`请前往：[PrometheusExporter](https://prometheus.io/docs/instrumenting/exporters/)

上面我们也说过有一些应用可能没有自带 `/metrics` 接口供 Prometheus 使用，在这种情况下，我们就需要利用 `exporter` 服务来为 Prometheus 提供指标数据了。Prometheus 官方为许多应用就提供了对应的 `exporter` 应用，也有许多第三方的实现，我们可以前往官方网站进行查看：[exporters](https://prometheus.io/docs/instrumenting/exporters/)，当然如果你的应用本身也没有 exporter 实现，那么就要我们自己想办法去实现一个 `/metrics` 接口了，只要你能提供一个合法的 `/metrics` 接口，Prometheus 就可以监控你的应用。

比如我们这里通过一个 [nginx-exporter](https://github.com/nginxinc/nginx-prometheus-exporter) 的服务来监控 redis 服务，对于这类应用，我们一般会以 `sidecar` 的形式和主应用部署在同一个 Pod 中，比如我们这里来部署一个nginx 应用，并用 nginx-exporter 的方式来采集监控数据供 Prometheus 使用，如下资源清单文件：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-work
  labels:
    app: nginx-work
spec:
  selector:
    matchLabels:
      app: nginx-work
  template:
    metadata:
      labels:
        app: nginx-work
    spec:
      containers:
      - name: nginx-work
        image: nginx:latest
        resources:
          requests:
            cpu: 10m
            memory: 200Mi
        ports:
          - name: http
            containerPort: 80
        volumeMounts:
          - name: nginx-config
            mountPath: /etc/nginx/conf.d
      - name: nginx-exporter-sidecar
        image: nginx/nginx-prometheus-exporter:0.10.0
        args:
        - '--nginx.scrape-uri=http://10.10.244.252/nginx_status'
        resources:
          requests:
            memory: 25Mi
        ports:
          - name: prom-http
            containerPort: 9113
      volumes:
      - name: nginx-config
        configMap:
          name: nginx-config
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-web
  labels:
    app: nginx-work
spec:
  selector:
    app: nginx-work
  ports:
  - name: nginx
    port: 80
    targetPort: 80
  - name: prom-http
    port: 9113
    targetPort: 9113
  - name: status
    port: 8080
    targetPort: 8080
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  default.conf: |
    server {
      listen       80;
      server_name  localhost;

      location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
      }
      location /nginx_status {
        stub_status on;
      }
    }
```

同样的我们现在只需要更新`prometheus`的`config`文件即可

```yaml
- job_name: 'nginx'
  static_configs:
  - targets: ['nginx-web:9113']
```

## 集群节点

对于集群的监控一般我们需要考虑以下几个方面：

- Kubernetes 节点的监控：比如节点的 cpu、load、disk、memory 等指标
- 内部系统组件的状态：比如 kube-scheduler、kube-controller-manager、kubedns/coredns 等组件的详细运行状态
- 编排级的 metrics：比如 Deployment 的状态、资源请求、调度和 API 延迟等数据指标

Kubernetes 集群的监控方案目前主要有以下几种方案：

- Heapster：Heapster 是一个集群范围的监控和数据聚合工具，以 Pod 的形式运行在集群中。 heapster 除了 Kubelet/cAdvisor 之外，我们还可以向 Heapster 添加其他指标源数据，比如 kube-state-metrics，需要注意的是 Heapster 已经被废弃了，后续版本中会使用 metrics-server 代替。
- cAdvisor：[cAdvisor](https://github.com/google/cadvisor) 是 Google 开源的容器资源监控和性能分析工具，它是专门为容器而生，本身也支持 Docker 容器。
- kube-state-metrics：[kube-state-metrics](https://github.com/kubernetes/kube-state-metrics) 通过监听 API Server 生成有关资源对象的状态指标，比如 Deployment、Node、Pod，需要注意的是 kube-state-metrics 只是简单提供一个 metrics 数据，并不会存储这些指标数据，所以我们可以使用 Prometheus 来抓取这些数据然后存储。
- metrics-server：metrics-server 也是一个集群范围内的资源数据聚合工具，是 Heapster 的替代品，同样的，metrics-server 也只是显示数据，并不提供数据存储服务。

不过 kube-state-metrics 和 metrics-server 之间还是有很大不同的，二者的主要区别如下：

- kube-state-metrics 主要关注的是业务相关的一些元数据，比如 Deployment、Pod、副本状态等
- metrics-server 主要关注的是[资源度量 API](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/instrumentation/resource-metrics-api.md) 的实现，比如 CPU、文件描述符、内存、请求延时等指标。

### 监控集群节点

要监控节点其实我们已经有很多非常成熟的方案了，比如 Nagios、zabbix，甚至我们自己来收集数据也可以，我们这里通过 Prometheus 来采集节点的监控指标数据，可以通过[node_exporter](https://github.com/prometheus/node_exporter) 来获取，顾名思义，`node_exporter` 就是抓取用于采集服务器节点的各种运行指标，目前 `node_exporter` 支持几乎所有常见的监控点，比如`conntrack，cpu，diskstats，filesystem，loadavg，meminfo，netstat`。

> 主要说一下`conntrack`这个东西,大家都不知道这个东西是干嘛的
>
> 1. Conntrack（Connection Tracking）是 Linux 内核中的一个功能模块，用于跟踪网络连接和协议会话。Conntrack 可以对 TCP、UDP、ICMP 等协议进行状态跟踪，记录每个连接的源 IP、目的 IP、源端口、目的端口等信息，并根据这些信息维护一个连接表，以便在数据包转发时快速查找和匹配已建立的连接。
> 2. Conntrack 的主要作用是实现 NAT 和防火墙等功能。在 NAT 方面，Conntrack 可以识别出内网主机与 Internet 之间的数据包，并自动映射地址和端口，以便数据包正确到达目标主机。在防火墙方面，Conntrack 可以根据连接状态和协议类型，对进出的数据包进行过滤和处理，以保证网络安全和可靠性。

我们可以通过 `DaemonSet` 控制器来部署该服务，这样每一个节点都会自动运行一个这样的 Pod，如果我们从集群中删除或者添加节点后，也会进行自动扩展。

```yaml
# prome-node-exporter.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  labels:
    app: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      volumes:
        - name: proc
          hostPath:
            path: /proc
            type: ''
        - name: sys
          hostPath:
            path: /sys
            type: ''
        - name: root
          hostPath:
            path: /
            type: ''
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.5.0
        args:
        - --web.listen-address=$(HOSTIP):9100
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --no-collector.hwmon # 禁用不需要的一些采集器
        - --no-collector.nfs
        - --no-collector.nfsd
        - --no-collector.nvme
        - --no-collector.dmi
        - --no-collector.arp
        - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/containerd/.+|/var/lib/docker/.+|var/lib/kubelet/pods/.+)($|/)
        - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$
        ports:
        - containerPort: 9100
        env:
        - name: HOSTIP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        resources:
          requests:
            cpu: 150m
            memory: 180Mi
          limits:
            cpu: '1'
            memory: 500Mi
        securityContext:
          runAsNonRoot: true
          runAsUser: 65534
        volumeMounts:
        - name: proc
          mountPath: /host/proc
        - name: sys
          mountPath: /host/sys
        - name: root
          mountPath: /host/root
          mountPropagation: HostToContainer
          readOnly: true
      restartPolicy: Always
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      hostNetwork: true
      hostPID: true
      hostIPC: true
      tolerations:
        - operator: Exists
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
```

由于我们要获取到的数据是主机的监控指标数据，而我们的 `node-exporter` 是运行在容器中的，所以我们在 Pod 中需要配置一些 Pod 的安全策略，这里我们就添加了 `hostPID: true`、`hostIPC: true`、`hostNetwork: true` 3个策略，用来使用主机的 `PID namespace`、`IPC namespace` 以及主机网络，这些 namespace 就是用于容器隔离的关键技术，要注意这里的 namespace 和集群中的 namespace 是两个完全不相同的概念。

另外我们还将主机的 `/dev`、`/proc`、`/sys`这些目录挂载到容器中，这些因为我们采集的很多节点数据都是通过这些文件夹下面的文件来获取到的，比如我们在使用 `top` 命令可以查看当前 cpu 使用情况，数据就来源于文件 `/proc/stat`，使用 `free` 命令可以查看当前内存使用情况，其数据来源是来自 `/proc/meminfo` 文件。

```bash
[root@Online-Beijing-master1 ~]# kubectl get daemonset
NAME            DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
node-exporter   6         6         6       6            6           kubernetes.io/os=linux   21m
```

### 服务发现

由于我们这里每个节点上面都运行了 `node-exporter` 程序，如果我们通过一个 Service 来将数据收集到一起用静态配置的方式配置到 Prometheus 去中，就只会显示一条数据，我们得自己在指标数据中去过滤每个节点的数据，当然我们也可以手动的把所有节点用静态的方式配置到 Prometheus 中去，但是以后要新增或者去掉节点的时候就还得手动去配置，那么有没有一种方式可以让 Prometheus 去自动发现我们节点的 `node-exporter` 程序，并且按节点进行分组呢？这就是 Prometheus 里面非常重要的**服务发现**功能了。

在 Kubernetes 下，Promethues 通过与 Kubernetes API 集成，主要支持5中服务发现模式，分别是：`Node`、`Service`、`Pod`、`Endpoints`、`Ingress`。

但是要让 Prometheus 也能够获取到当前集群中的所有节点信息的话，我们就需要利用 Node 的服务发现模式，同样的，在 `prometheus.yml` 文件中配置如下的 job 任务即可

```yaml
- job_name: 'nodes'
  kubernetes_sd_configs:
  - role: node
```

重新加载`prometheus`后会出现如下问题,你会发现节点状态都是`down`

| Endpoint                       | State    | Labels                                                       | Last Scrape | Scrape Duration | Error                                       |
| ------------------------------ | -------- | ------------------------------------------------------------ | ----------- | --------------- | ------------------------------------------- |
| http://10.1.6.47:10250/metrics | **DOWN** | **instance="online-beijing-node3"****job="kubernetes-cluster"** | 7.961s ago  | 0.698ms         | server returned HTTP status 400 Bad Request |
| http://10.1.6.48:10250/metrics | **DOWN** | **instance="online-beijing-master1"****job="kubernetes-cluster"** | 3.944s ago  | 0.632ms         | server returned HTTP status 400 Bad Request |
| http://10.1.6.24:10250/metrics | **DOWN** | **instance="online-beijing-master2"****job="kubernetes-cluster"** | 1.453s ago  | 0.628ms         | server returned HTTP status 400 Bad Request |
| http://10.1.6.45:10250/metrics | **DOWN** | **instance="online-beijing-master3"****job="kubernetes-cluster"** | 3.445s ago  | 0.661ms         | server returned HTTP status 400 Bad Request |
| http://10.1.6.46:10250/metrics | **DOWN** | **instance="online-beijing-node1"****job="kubernetes-cluster"** | 6.722s ago  | 0.607ms         | server returned HTTP status 400 Bad Request |
| http://10.1.6.43:10250/metrics | **DOWN** | **instance="online-beijing-node2"****job="kubernetes-cluster"** | 10.127s ago | 0.467ms         | server returned HTTP status 400 Bad Request |

我们可以看到上面的 `nodes` 这个 job 任务已经自动发现了我们 node 节点但是在获取数据的时候失败了，出现了类似于下面的错误信息：

```shell
server returned HTTP status 400 Bad Request
```

这个是因为 prometheus 去发现 Node 模式的服务的时候，访问的端口默认是 10250，而默认是需要认证的 https 协议才有权访问的，但实际上我们并不是希望让去访问10250端口的 `/metrics` 接口，而是 `node-exporter` 绑定到节点的 9100 端口，所以我们应该将这里的 `10250` 替换成 `9100`

这里我们就需要使用到 Prometheus 提供的 `relabel_configs` 中的 `replace` 能力了，`relabel` 可以在 Prometheus 采集数据之前，通过 Target 实例的 `Metadata` 信息，动态重新写入 Label 的值。除此之外，我们还能根据 Target 实例的 `Metadata` 信息选择是否采集或者忽略该 Target 实例。比如我们这里就可以去匹配 `__address__` 这个 Label 标签，然后替换掉其中的端口，如果你不知道有哪些 Label 标签可以操作的话，可以在 `Service Discovery` 页面获取到相关的元标签，这些标签都是我们可以进行 Relabel 的标签：

```yaml
- job_name: 'cluster'
  kubernetes_sd_configs:
  - role: node
  relabel_configs:
  - source_labels: [__address__]
    regex: '(.*):10250'
    replacement: '${1}:9100'
    target_label: __address__
    action: replace
```

这里就是一个正则表达式，去匹配 `__address__` 这个标签，然后将 host 部分保留下来，port 替换成了 9100，现在我们重新更新配置文件，执行 reload 操作.

这里我们可以通过 `labelmap` 这个属性来将 Kubernetes 的 Label 标签添加为 Prometheus 的指标数据的标签：

```yaml
- job_name: 'cluster'
  kubernetes_sd_configs:
  - role: node
  relabel_configs:
  - source_labels: [__address__]
    regex: '(.*):10250'
    replacement: '${1}:9100'
    target_label: __address__
    action: replace
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)
```

添加了一个 action 为 `labelmap`，正则表达式是 `__meta_kubernetes_node_label_(.+)` 的配置，这里的意思就是表达式中匹配都的数据也添加到指标数据的 Label 标签中去。

对于 `kubernetes_sd_configs` 下面可用的元信息标签如下：

- `__meta_kubernetes_node_name`：节点对象的名称
- `_meta_kubernetes_node_label`：节点对象中的每个标签
- `_meta_kubernetes_node_annotation`：来自节点对象的每个注释
- `_meta_kubernetes_node_address`：每个节点地址类型的第一个地址（如果存在）

> 关于 kubernets_sd_configs 更多信息可以查看官方文档：[kubernetes_sd_config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#)

## 容器监控

说到容器监控我们自然会想到 `cAdvisor`，我们前面也说过 cAdvisor 已经内置在了 kubelet 组件之中，所以我们不需要单独去安装，`cAdvisor` 的数据路径为 `/api/v1/nodes/<node>/proxy/metrics`，但是我们不推荐使用这种方式，因为这种方式是通过 APIServer 去代理访问的，对于大规模的集群比如会对 APIServer 造成很大的压力，所以我们可以直接通过访问 kubelet 的 `/metrics/cadvisor` 这个路径来获取 cAdvisor 的数据， 同样我们这里使用 node 的服务发现模式，因为每一个节点下面都有 kubelet，自然都有 `cAdvisor` 采集到的数据指标，配置如下：

### 第一种方式



```yaml
- job_name: 'cadvisor'
  kubernetes_sd_configs:
  - role: node
  tls_config:
    ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    insecure_skip_verify: true
  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
  relabel_configs:
  - action: labelmap
    regex: __meta_kubernetes_node_label_(.+)
    replacement: $1
  - source_labels: [__meta_kubernetes_node_name]
    regex: (.+)
    replacement: /metrics/cadvisor    # <nodeip>/metrics -> <nodeip>/metrics/cadvisor
    target_label: __metrics_path__
```



### 第二种方式



不直接通过kubelet的metrics服务采集监控数据，而通过Kubernetes的api-server提供的代理API访问各个节点中kubelet的metrics服务，如下所示：



```yaml
- job_name: 'kubernetes-kubelet'
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__address__]__meta_kubernetes_node_name]
        regex: '(.*+):10250'
        target_label: __metrics_path__
        replacement: '/api/v1/nodes/${1}:9100'
    target_label: __address__
    action: replace/proxy/metrics
```



通过relabeling，将从Kubernetes获取到的默认地址`__address__`替换为kubernetes.default.svc:443。同时将`__metrics_path__`替换为api-server的代理地址/api/v1/nodes/${1}/proxy/metrics。

> 注意：请务必确保你的`Prometheus`的RBAC具有操作`nodes/metrics`的权限。

`container_cpu_usage_seconds_total` 是容器累计使用的 CPU 时间，用它除以 CPU 的总时间，就可以得到容器的 CPU 使用率了：

首先计算容器的 CPU 占用时间，由于节点上的 CPU 有多个，所以需要将容器在每个 CPU 上占用的时间累加起来，Pod 在 1m 内累积使用的 CPU 时间为：(根据 pod 和 namespace 进行分组查询)

```sql
sum(rate(container_cpu_usage_seconds_total{image!="",pod!=""}[1m])) by (namespace, pod)
```

然后计算 CPU 的总时间，这里的 CPU 数量是容器分配到的 CPU 数量，`container_spec_cpu_quota` 是容器的 CPU 配额，它的值是容器指定的 `CPU 个数 * 100000`，所以 Pod 在 1s 内 CPU 的总时间为：Pod 的 CPU 核数 * 1s：

```sql
sum(container_spec_cpu_quota{image!="", pod!=""}) by(namespace, pod) / 100000
```

## 结合Grafana

Grafana 是一个可视化面板，有着非常漂亮的图表和布局展示，功能齐全的度量仪表盘和图形编辑器，支持 Graphite、zabbix、InfluxDB、Prometheus、OpenTSDB、Elasticsearch 等作为数据源，比 Prometheus 自带的图表展示功能强大太多，更加灵活，有丰富的插件，功能更加强大。

同样的我们将 grafana 安装到 Kubernetes 集群中，第一步去查看 grafana 的 docker 镜像的介绍，我们可以在 dockerhub 上去搜索，也可以在官网去查看相关资料，镜像地址如下：https://hub.docker.com/r/grafana/grafana/，我们可以看到介绍中运行 grafana 容器的命令非常简单：

```shell
docker run -d --name=grafana -p 3000:3000 grafana/grafana
```

一般我们选择使用`statefulset`的方式进行部署并且持久化数据

```yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana-k8s
spec:
  selector:
    app: grafana-k8s
  ports:
    - name: web
      nodePort: 30002
      port: 3000
      targetPort: 3000
      protocol: TCP
  clusterIP: None
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: grafana-k8s
  labels:
    app: grafana-k8s
spec:
  replicas: 1
  serviceName: grafana-k8s
  selector:
    matchLabels:
      app: grafana-k8s
  template:
    metadata:
      labels:
        app: grafana-k8s
    spec:
      serviceAccount: prometheus
      containers:
        - name: grafana
          image: grafana/grafana:latest
          ports:
            - name: web
              containerPort: 3000
          env:
            - name: GF_SECURITY_ADMIN_USER
              value: admin
            - name: GF_SECURITY_ADMIN_PASSWORD
              value: admin321
          volumeMounts:
            - name: grafana-data
              mountPath: /var/lib/grafana
          readinessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 30
          livenessProbe:
            failureThreshold: 3
            httpGet:
              path: /api/health
              port: 3000
              scheme: HTTP
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 10
          resources:
            requests:
              memory: "400Mi"
              cpu: "100m"
  volumeClaimTemplates:
    - metadata:
        name: grafana-data
        annotations:
          volume.beta.kubernetes.io/storage-class: managed-nfs-storage
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 5Gi
```

### 安装DevOpsProdigy KubeGraf

Grafana App for Kubernetes 插件 （https://grafana.com/plugins/grafana-kubernetes-app） 的更新版本，该插件允许您可视化和分析 Kubernetes 集群的性能。它以图形形式展示了 Kubernetes 集群的主要服务指标和特征。它还使检查应用程序的生命周期和错误日志变得更加容易。

> 注意：貌似这个好像只支持7.x的版本，不支持8.x以上的...

1. 进入grafana

```bash
[root@Online-Beijing-master1 ~]# kubectl exec -it grafana-k8s-0 /bin/bash
```

2. 安装`kubeGraf`

```bash
bash-5.1$ grafana-cli plugins install devopsprodigy-kubegraf-app
installing devopsprodigy-kubegraf-app @ 1.5.2
from: https://grafana.com/api/plugins/devopsprodigy-kubegraf-app/versions/1.5.2/download
into: /var/lib/grafana/plugins

✔ Installed devopsprodigy-kubegraf-app successfully 
installing grafana-piechart-panel @ 1.6.4
from: https://grafana.com/api/plugins/grafana-piechart-panel/versions/1.6.4/download
into: /var/lib/grafana/plugins

✔ Installed grafana-piechart-panel successfully 
Installed dependency: grafana-piechart-panel ✔

Restart grafana after installing plugins . <service grafana-server restart>
```

3. 查看你的`~/.kube/config`当中的`kubernetes-admin`，分别去`base64`解码你的CA和你的KEY

```yaml
users:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURJVENDQWdtZ0F3SUJBZ0lJZU1EaTZ1SkNlQUl3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TXpBeU1EWXhNekV4TkRkYUZ3MHlOREF5TURZeE16RXhORGhhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQTEycW53SjUxT0F4eloyMC8KU0IvYitpU3liMG80ZjBOK0hObzk2UlZaajkwb0xVSDZtUERGdjFOZWtseFZIRUNIeEx5RHF2bThzbFdCTDBPaApWVWpFdUNEeEtmYStSQUhhYXQzc1hNalFhMUVZdE1KSDJweWZCYXpOU2xTYTdXMGY3cllGaWtJSFJGUHp0c1E0Cll0WmpLRXlsalZsaGdTZDIxcGpaMmQwRTlpZk5pMEtlKzlLd0hXWkI3YlcxQWV2Y1EvUEdqQS9UWXhFeVBTNGEKMGFLdmJQUXdFL0JkclNGWHVKbitxMHhjOTlxdFNMUy94R3RVeG5ncHIvSmk4UkRYK3JzZEJUdmxiOFRwQnU0dgoxbDdsekUwOTJtSlhCeG9CVDY0ampOSnE3UTRXbEhaT2E4K0psR1FWN251RWlpK2FVaHR2dmtzcEN5L3g3ckc1CmlibE90d0lEQVFBQm8xWXdWREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0RBWURWUjBUQVFIL0JBSXdBREFmQmdOVkhTTUVHREFXZ0JTZFZGakRRM204SUcwOHl1Tkl1Y1orL1l4MApoekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBc1ZGVDJpZEZza1RJOFA3MXhxSjN6UXlVaFA3SWhRTkdLUnVhCjU2bHpFWGkvYjRkLzdMZE9jRk1vdmJvWUd3NDlvdmdrUzhleE43UGNJbi9qUXJMRHZKQSs5VWRYUnFwU3B4VGwKU0lGWTc1dW5uaE5BeWNxbUdIeWs4QmsxQ2pHRjFVcEpWMzFWc2RNUU5MdTNoemMvLzJDRFRGWUNzT2o1T3VvMQpBRVRNdFRqQ2FlYm1jK2xNdzV2WGtiOVBxMXZrVlg0Mks5VHZOTmZyeWNOVkNCeUdJNkJtUnZ1eXFuZ2xOU0ZqCjBFSjRDOW1iMUtXbXVYNDVia0tHRDhGV09GSC95eFhNR0lQckFNN0xQcktjc1QyWXE3cmhndi9BY1dOMklObmkKME5QYXBvR1RxN0UvV1FDeVpDZVArK0JJTDdZeWZnWXVNbzhVaitWQjdHdVZhajVNOFE9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
    client-key-data: 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
```



### 自定义图表

然后在 `Metrics` 区域输入我们要查询的监控 PromQL 语句，比如我们这里想要查询集群节点 CPU 的使用率：

```text
(1 - sum(increase(node_cpu_seconds_total{mode="idle", instance=~"$node"}[1m])) by (instance) / sum(increase(node_cpu_seconds_total{instance=~"$node"}[1m])) by (instance)) * 100
```